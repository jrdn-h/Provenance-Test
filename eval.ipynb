{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d382a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import random\n",
    "import re\n",
    "import json\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI, RateLimitError, APIError, Timeout\n",
    "\n",
    "load_dotenv()\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "df = pd.read_csv(\"ground_truth.csv\")\n",
    "\n",
    "SYS_MSG = \"\"\"\n",
    "You are an expert provenance auditor.\n",
    "\n",
    "Return **ONLY** a JSON object that follows this exact schema\n",
    "{\n",
    "  \"class\"       : \"LoanDoc | PublicWeb | Synthesised\",\n",
    "  \"confidence\"  : 0-100,                  // subjective probability\n",
    "  \"loan_doc_id\" : \"file_name.pdf\"         // required only if class = LoanDoc\n",
    "}\n",
    "\n",
    "──────────────── DECISION RULES ────────────────\n",
    "1. **LoanDoc** Verbatim or lightly edited excerpt from any internal loan file in the list below.\n",
    "2. **PublicWeb** Appears on freely available websites, blogs, press releases, gov/data portals.\n",
    "3. **Synthesised** Blends or paraphrases two or more sources, or was written from scratch by an AI.\n",
    "\n",
    "If you choose **\"LoanDoc\"**, set `loan_doc_id` to the best match from:\n",
    "\n",
    "[TS1.pdf]   Term Sheet  \n",
    "[APP1.pdf]  Appraisal  \n",
    "[OM1.pdf]   Offering Memo  \n",
    "\n",
    "(Leave `loan_doc_id` as an empty string for all other classes.)\n",
    "\n",
    "OUTPUT POLICY  \n",
    "• No extra keys, comments, or prose - **only** the JSON object.\n",
    "\"\"\".strip()\n",
    "\n",
    "FEW_SHOT = \"\"\"\n",
    "Example 1\n",
    "<!-- internal reasoning: appraisal language + dollar value should map to LoanDoc -->\n",
    "{\"class\":\"LoanDoc\",\"confidence\":92,\"loan_doc_id\":\"APP1.pdf\"}\n",
    "\n",
    "Example 2\n",
    "<!-- internal reasoning: covenant with $ amounts belongs in a term sheet -->\n",
    "{\"class\":\"LoanDoc\",\"confidence\":91,\"loan_doc_id\":\"TS1.pdf\"}\n",
    "\n",
    "Example 3\n",
    "<!-- internal reasoning: covenant-style clause appears only in term sheets -->\n",
    "{\"class\":\"LoanDoc\",\"confidence\":92,\"loan_doc_id\":\"TS1.pdf\"}\n",
    "\n",
    "Example 4\n",
    "<!-- internal reasoning: appraisal narrative language; no dollar or percent signs -->\n",
    "{\"class\":\"LoanDoc\",\"confidence\":90,\"loan_doc_id\":\"APP1.pdf\"}\n",
    "\n",
    "Example 5\n",
    "<!-- internal reasoning: generic definition easily found on public finance sites -->\n",
    "{\"class\":\"PublicWeb\",\"confidence\":85,\"loan_doc_id\":\"\"}\n",
    "\n",
    "Example 6\n",
    "<!-- internal reasoning: blends Fannie-Mae DSCR guideline with sponsor equity requirement; not verbatim from any one doc -->\n",
    "{\"class\":\"Synthesised\",\"confidence\":80,\"loan_doc_id\":\"\"}\n",
    "\"\"\".strip()\n",
    "COT = \"\"\"\n",
    "Think silently, then output ONLY the JSON.\n",
    "\n",
    "<!--\n",
    "STEP 1: Identify surface cues  \n",
    "- origin hints (numbers? legal verbs? valuation jargon?)  \n",
    "- presence of web tell-tales (URL, “according to”, etc.)  \n",
    "- any words unique to TS1, APP1, OM1\n",
    "\n",
    "STEP 2: Decide provisional class + doc_id  \n",
    "- choose class LoanDoc / PublicWeb / Synthesised  \n",
    "- if LoanDoc: pick TS1 / APP1 / OM1 or \"\" when unsure  \n",
    "- note confidence 0-100\n",
    "\n",
    "STEP 3: Self-check  \n",
    "- Does evidence contradict class?  \n",
    "- If confidence <60, consider next-best label.  \n",
    "\n",
    "END STEPS  \n",
    "→\n",
    "\"\"\".strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fb7a9b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_prompt(variant: str, snippet: str) -> list[dict]:\n",
    "    \"\"\"Construct the messages list expected by the Chat Completions API.\"\"\"\n",
    "    messages = [{\"role\": \"system\", \"content\": SYS_MSG}]\n",
    "\n",
    "    if variant == \"fewshot\":\n",
    "        user_msg = FEW_SHOT + f'\\nText: \"\"\"{snippet}\"\"\"\\nPrediction:'\n",
    "    elif variant == \"zeroshot\":\n",
    "        user_msg = f'Text: \"\"\"{snippet}\"\"\"\\nPrediction:'\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown variant: {variant}\")\n",
    "\n",
    "    messages.append({\"role\": \"user\", \"content\": user_msg})\n",
    "    return messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0f68a5d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_chat_completion(\n",
    "    messages: list[dict],\n",
    "    variant: str,\n",
    "    model: str = \"gpt-4.1-2025-04-14\",\n",
    "    max_retries: int = 6,\n",
    "    base_delay: float = 1.0,\n",
    "):\n",
    "    \"\"\"Robust wrapper around `client.chat.completions.create` with back-off.\"\"\"\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            kwargs = dict(model=model, messages=messages, temperature=0)\n",
    "\n",
    "            if variant in (\"zeroshot\", \"fewshot\"):\n",
    "                # Force JSON‑only replies\n",
    "                kwargs[\"response_format\"] = {\"type\": \"json_object\"}\n",
    "\n",
    "            return client.chat.completions.create(**kwargs)\n",
    "\n",
    "        except (RateLimitError, APIError, Timeout) as e:\n",
    "            if attempt == max_retries - 1:\n",
    "                raise\n",
    "            wait = base_delay * (2 ** attempt) + random.uniform(0, 1)\n",
    "            print(f\"{type(e).__name__}: retrying in {wait:.1f}s…\")\n",
    "            time.sleep(wait)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "625c7605",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Completed — results saved to run_log.csv\n"
     ]
    }
   ],
   "source": [
    "records = []\n",
    "variants = [\"fewshot\"]  # Add \"zeroshot\" here if you want to compare\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    sample_id = row.get(\"id\", idx)\n",
    "    snippet = row[\"snippet\"]\n",
    "    true_lbl = row[\"label\"]\n",
    "\n",
    "    for variant in variants:\n",
    "        messages = make_prompt(variant, snippet)\n",
    "        rsp = call_chat_completion(messages, variant)\n",
    "\n",
    "        raw = rsp.choices[0].message.content.strip()\n",
    "\n",
    "        # Robust JSON extraction (handles stray text)\n",
    "        try:\n",
    "            out = json.loads(raw)\n",
    "        except json.JSONDecodeError:\n",
    "            m = re.search(r\"\\{.*?\\}\", raw, re.S)\n",
    "            if not m:\n",
    "                print(f\"⚠ No JSON for id={sample_id}, variant={variant}\\n{raw[:120]}…\")\n",
    "                continue\n",
    "            out = json.loads(m.group())\n",
    "\n",
    "        records.append({\n",
    "            \"id\": sample_id,\n",
    "            \"prompt_variant\": variant,\n",
    "            \"ground_truth\": true_lbl,\n",
    "            \"predicted\": out.get(\"class\"),\n",
    "            \"confidence\": out.get(\"confidence\"),\n",
    "            \"loan_doc_pred\": out.get(\"loan_doc_id\", \"\"),\n",
    "            \"raw_response\": raw,\n",
    "        })\n",
    "\n",
    "results = pd.DataFrame(records)\n",
    "results.to_csv(\"run_log.csv\", index=False)\n",
    "print(\"✓ Completed — results saved to run_log.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c3ace96a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     LoanDoc      1.000     0.625     0.769         8\n",
      "   PublicWeb      1.000     1.000     1.000         6\n",
      " Synthesised      0.667     1.000     0.800         6\n",
      "\n",
      "    accuracy                          0.850        20\n",
      "   macro avg      0.889     0.875     0.856        20\n",
      "weighted avg      0.900     0.850     0.848        20\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LoanDoc</th>\n",
       "      <th>PublicWeb</th>\n",
       "      <th>Synthesised</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>LoanDoc</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PublicWeb</th>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Synthesised</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             LoanDoc  PublicWeb  Synthesised\n",
       "LoanDoc            5          0            3\n",
       "PublicWeb          0          6            0\n",
       "Synthesised        0          0            6"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "print(classification_report(results[\"ground_truth\"], results[\"predicted\"], digits=3))\n",
    "\n",
    "cm = pd.DataFrame(\n",
    "    confusion_matrix(\n",
    "        results[\"ground_truth\"],\n",
    "        results[\"predicted\"],\n",
    "        labels=sorted(results[\"ground_truth\"].unique()),\n",
    "    ),\n",
    "    index=sorted(results[\"ground_truth\"].unique()),\n",
    "    columns=sorted(results[\"ground_truth\"].unique()),\n",
    ")\n",
    "cm.to_csv(\"confusion_matrix.csv\")\n",
    "cm\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
